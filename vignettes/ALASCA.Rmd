---
title: "Get Started"
author: "Anders H. Jarmund"
date: "`r Sys.Date()`"
bibliography: ALASCA.bib
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Get Started}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  out.width = "100%", 
  fig.asp = 0.7,
  fig.width = 12,
  fig.align = "center",
  cache = FALSE,
  external = FALSE
)
```

# Introduction to ALASCA

<script src="js/hideOutput.js"></script>
<style>
.showopt {
  background-color: #004c93;
  color: #FFFFFF; 
  width: 150px;
  height: 20px;
  text-align: center;
  vertical-align: middle !important;
  float: left;
  font-family: sans-serif;
  border-radius: 8px;
}

.showopt:hover {
    background-color: #dfe4f2;
    color: #004c93;
}

.showopttext::before{
  content: "Show/Hide Source"
}

pre.plot {
  background-color: white !important;
}
</style>

```{r setup}
library(ALASCA)
library(ggplot2)
```

This vignette demonstrates the basic functionality of the [ALASCA package](https://andjar.github.io/ALASCA), with **repeated measures** (`RMASCA()`). For more detailed case studies, see

* For analysis of **interventional**, see [the vignette on metabolomics](metabolomics.html)
* For analysis of **observational** data, see [the vignette on early preeclampsia](pregnancy.html)
* For analysis of single measures data, see [the vignette on personality](personality.html)

In general, you need to start by deciding what kind of model you want to set up. ALASCA builds on linear mixed models, and you are required to have at least one random intercept (usually participant). You need to decide if you want to separate time and group effects (`separateTimeAndGroup = TRUE/FALSE`) and, if you are working with interventional data, whether you want a common baseline between groups (`forceEqualBaseline = TRUE`).

Next, you need to think through whether you want to scale your data. If not, use `scaleFun = FALSE`, but default is `scaleFun = TRUE` corresponding to z score transformation. Often you want to use a custom scaling function, for example mean centering but dividing by the standard deviation at baseline. The custom scaling function, eg. `foo(df)` should have your data frame as both input and output and be provided to `ALASCA()` (`scaleFun = foo`). You should *not* do the scaling yourself as this can result in information leak during validation, which should be avoided.

<div class="alert alert-info">
<strong>How do I know what scaling to use?</strong> There is no straight answer, but have a look at @timmermanScalingANOVAsimultaneousComponent2015.
</div>

Your data should be organized in a long-format data frame where

* one column contains participant IDs, and the column should be specified with `participantColumn` (eg. `participantColumn = "ID"`)
* one column is called `time` and contains categorical time points
* one column is called `group` and contains group belonging
* one column is called `variable` and contains the named of the measured variable
* one column contains measured values. The name of this column is automatically extracted from the model you set up.

<div class="alert alert-info">
<strong>What does long format mean?</strong> Sometimes we talk about *long* vs *wide* format. In *wide* format, each measured variable would have its own column, eg. BMI, glucose, age etc. but in *long* format, we have a single column for variable name and a single column for the corresponding values. To convert between them, have a look at the `melt()` (wide -> long) and `reshape2::dcast()` (long -> wide) functions.
</div>

You can, of course, have more columns. In the following is some key concepts demonstrated. You may also want have a look at @madssenRepeatedMeasuresASCA2020a.

## Creating a dummy data set

Let us start out by creating some dummy data for us to work with. In this example we have 600 participants measured four different times. The participants belonged to three different groups -- for example control group, chocolate diet and salad diet -- and we measured ten variables each time.

<div class="fold s">
```{r dummy_data_configuration}
nPart <- 600
nGroups <- c("Controls", "Chocolate", "Salad")
nTime <- c(1, 2, 3, 4)
variables <- c("BMI", "Glucose", "VLDL", "LDL", "HDL", "ferritin", "CRP", "Happiness", "Anger", "Age")
```
</div>

The `ALASCA()` function expects a data frame in long format that contains at least one column for time (called `time`), one for group (called `group`) and one for variables (called `variable`).

<div class="fold s">
```{r dummy_data}
gr <- nGroups[sample(c(1:length(unique(nGroups))), nPart, replace = TRUE)]
df <- Reduce(rbind, lapply(nTime, function(x){
  data.frame(
    partid = c(1:nPart),
    group = gr,
    time = rep(x, nPart)
  )
}))
for(i in 1:length(variables)){
  df[,3+i] <- rnorm(nrow(df), mean = 10, sd = 3)
}
colnames(df) <- c("partid", "group", "time", variables)
```
</div>

Random data aren't very interesting, so let us add some trends.

* Control group: These are boring and serve as reference
* Chocolate group: Let us imagine they responded something like
  * Increasing BMI and glucose
  * Initially decreased anger and increased happiness, but only at time 2 and 3 (before the stomach begins to hurt...)
* Salad group: Let us imagine they responded something like
  * Decreasing BMI and glucose
  * Initially increased anger and decreased happiness, but only at time 2 and 3 before they get used to it and start to enjoy their new life

The age, however, increases for all groups, and HDL is in this case a function of age, and the other variables are unaffected and vary at random.

<div class="fold s">
```{r dummy_data_trends}
df$Anger[df$group == "Chocolate" & df$time == 2] <- df$Anger[df$group == "Chocolate" & df$time == 1]/2
df$Anger[df$group == "Chocolate" & df$time == 3] <- df$Anger[df$group == "Chocolate" & df$time == 1]/1.5
df$Happiness[df$group == "Chocolate" & df$time == 2] <- df$Happiness[df$group == "Chocolate" & df$time == 1]*2
df$Happiness[df$group == "Chocolate" & df$time == 3] <- df$Happiness[df$group == "Chocolate" & df$time == 1]*1.5
df$BMI[df$group == "Chocolate"] <- df$BMI[df$group == "Chocolate"]*df$time[df$group == "Chocolate"]
df$Glucose[df$group == "Chocolate"] <- df$Glucose[df$group == "Chocolate"]*df$time[df$group == "Chocolate"]

df$Anger[df$group == "Salad" & df$time == 2] <- df$Anger[df$group == "Salad" & df$time == 1]*2
df$Anger[df$group == "Salad" & df$time == 3] <- df$Anger[df$group == "Salad" & df$time == 1]*1.5
df$Happiness[df$group == "Salad" & df$time == 2] <- df$Happiness[df$group == "Salad" & df$time == 1]/2
df$Happiness[df$group == "Salad" & df$time == 3] <- df$Happiness[df$group == "Salad" & df$time == 1]/1.5
df$BMI[df$group == "Salad"] <- df$BMI[df$group == "Salad"]/df$time[df$group == "Salad"]
df$Glucose[df$group == "Salad"] <- df$Glucose[df$group == "Salad"]/df$time[df$group == "Salad"]

df$Age <- df$Age+df$time
df$HDL <- df$Age*1.2
```
</div>

And we need to define the control group and transform it to the long format. And since they are "real" people, we will give them individual baselines so that we need to use linear mixed models with participant as random intercepts.

<div class="fold s">
```{r data_dummy_melt}
df$group <- factor(df$group)
df$group <- relevel(df$group, ref = "Controls")
df <- reshape2::melt(df, id.vars = c("partid", "group", "time"))
for(i in unique(df$variables)){
  for(j in unique(df$partid)){
    df$value[df$variables == i & df$partid == j] <- df$value[df$variables == i & df$partid == j] + sample(0:5,1)
  }
}
```
</div>

Let us see how it looks at group level

<div class="fold s">
```{r data_dummy_plot}
ggplot(data = df, aes(x = factor(time), y = value, color = group)) +
  geom_boxplot() +
  facet_wrap(~variable, scales = "free_y") +
  theme(legend.position = "bottom") +
  labs(x = "Time", y = "Value", color = "Group")
```
</div>

And we can have a look at the participants using the `plotParts()` function,

<div class="fold s">
```{r data_dummy_plot_2}
do.call(
  ggpubr::ggarrange,
  c(plotParts(df, participantColumn = "partid", valueColumn = "value", addSmooth = NA), 
                      common.legend = TRUE, legend = "bottom")
  )
```
</div>

## Using RM-ASCA+

Now that we have a data set, let us try the `ALASCA()` function! First we need to decide what kind of model we want. For simplicity, let us start with a very simple model with terms for time, group and interaction.

```{r ALASCA_simple_model}
model.formula <- value ~ time*group + (1|partid)
res.simple <- ALASCA(df = df, formula = model.formula)
```

<div class="alert alert-info">
<strong>What does "Scaling data" mean?</strong> It is often necessary to rescale your data, and `ALASCA()` defaults to transforming each variable to z scores. You can turn this off by setting `scaleFun = FALSE`. You can also define your own scaling function, `foo <- function(df)` and provide it as an argument `scaleFun = foo`. You should *not* do such scaling beforehand (although simple log transformations are okay) since the validation involves selecting subsets from your data and the variables need to be scaled based on that subset to prevent data leak into you validation data sets.
</div>

<div class="alert alert-info">
<strong>What is meant by formula?</strong> ALASCA builds on linear mixed models, and you need to specify a regression model that includes at least one random intercept. In R, a regression formula typically looks like `y ~ x1 + x2` that will result in an intercept and a beta coefficient for `x1` and `x2`. For `ALASCA()`, you will often use a formula like `value ~ time*group + (1|participantID)`. Here, `participantID` represents the random intercept, whereas `time*group` is shorthand for `time + group + time:group` -- that is, main effects for time and group, and their interaction (`time:group`). You can also specify *only* time and interaction if you think that there are no main group effect by using `value ~ time + time:group + (1|participantID)`. Note, however, that there is an interaction term between group and first time point that you can remove by specifying `forceEqualBaseline = TRUE`. Various models are demonstrated below and in the case studies for [metabolomics](metabolomics.html) and [preeclampsia](pregnancy.html).
</div>

To visualize the result, simply use `plot(res.simple)`;

```{r ALASCA_simple_model_plot_PC1}
plot(res.simple)
```
Let us interpret the figure from top left:

* Top left: The control group has some variable -- one or more -- that increases steadily
* Top right: The variables that increase are age and HDL
* Bottom left: Relative to the control group, the chocolate group increases in some variable and the salad group decreases
* Bottom right: The variables that increase for the chocolate group (and decreases for the salad group) are mainly BMI and glucose

However, that is only the first component. In this case, that variable alone explains most of the variation in the data (>99%). Let us plot the second component too:

```{r ALASCA_simple_model_plot_PC2}
plot(res.simple, component = 2)
```

The interpretation is similar to the one above. We are most interested in the bottom panels: the chocolate group has first an increase at time 2, but then decrease again. To the right, we can see that happiness follows that trajectory, whereas anger has the inverted trajectory (first decreased anger, then return to baseline). The salad group has the opposite trajectories. Cool, huh?

We can explore this by visualizing predictions on group level from the underlying linear mixed models. For example

<div class="fold s">
```{r ALASCA_simple_model_plotPred}
do.call(
  ggpubr::ggarrange,
  c(plotPred(res.simple, variable = c("BMI", "Age", "Happiness", "Anger")), 
                      common.legend = TRUE, legend = "bottom")
  )
```
</div>

<div class="alert alert-info">
<strong>Why is the y axis so strange?</strong> Because the linear mixed models are built on *scaled* data.
</div>

#### Scree plots

So, how many components do we need? With PCA, the first component is always the one explaining most variance. We can look at how much the various components explain,

```{r ALASCA_screeplot}
screeplot(res.simple)
```

And it seems that we should use PC1 for the time effect, but probably both PC1 and PC2 for the group effect. Which makes sense, since we added several distinct group trends to our data that needs different trajectories to be explained.

### Customized plots

Often, we want some more control over the plots. Luckily, we can easily get the necessary data using `getLoadings()` and `getScores()`. For example

<div class="fold s">
```{r ALASCA_custom_plot}
# scores <- getScores(res.simple)
# loadings <- getLoadings(res.simple)
# tplot_score <- function(df, x, y, group, color = NA){
#   if(is.na(color)){
#     g <- ggplot(df, aes(x = x, y = y, group = group)) +
#       geom_point() + 
#       geom_line() + 
#       theme_bw()
#   }else{
#     g <- ggplot(df, aes(x = x, y = y, group = group, color = color)) +
#       geom_point() + 
#       geom_line() + 
#       theme_bw()
#   }
#   
#   return(g)
# }
# 
# tplot_loading <- function(df, x, y){
#   g <- ggplot(df, aes(x = x, y = y)) + 
#   geom_point() + 
#   theme_bw() +
#   theme(axis.text.x = element_text(angle = 90, vjust = 0.4, hjust=1))
#   return(g)
# }
# 
# sc <- subset(scores$time, PC == 1)
# gst_1 <- tplot_score(df = sc, x = sc$time, y = sc$score, group = NA) +
#   labs(x = "Time", y = paste0("Pr Comp. 1 (",round(100*scores$explained$time[1], 2),"%)"))
#   
# sc <- subset(scores$time, PC == 2)
# gst_2 <- tplot_score(df = sc, x = sc$time, y = sc$score, group = NA) + 
#   labs(x = "Time", y = paste0("Pr Comp. 2 (",round(100*scores$explained$time[2], 2),"%)"))
# 
# sc <- subset(scores$group, PC == 1)
# gsg_1 <- tplot_score(df = sc, x = sc$time, y = sc$score, group = sc$group, color = sc$group) + 
#   labs(x = "Time", y = paste0("Pr Comp. 1 (",round(100*scores$explained$group[1], 2),"%)"))
# 
# sc <- subset(scores$group, PC == 2)
# gsg_2 <- tplot_score(df = sc, x = sc$time, y = sc$score, group = sc$group, color = sc$group) + 
#   labs(x = "Time", y = paste0("Pr Comp. 2 (",round(100*scores$explained$group[2], 2),"%)"))
# 
# l <- subset(loadings$time, PC == 1)
# glt_1 <- tplot_loading(df = l, x = l$covars, y = l$loading) + 
#   labs(x = "Variable", y = paste0("Pr Comp. 1 (",round(100*loadings$explained$time[1], 2),"%)"))
# 
# l <- subset(loadings$time, PC == 2)
# glt_2 <- tplot_loading(df = l, x = l$covars, y = l$loading) + 
#   labs(x = "Variable", y = paste0("Pr Comp. 2 (",round(100*loadings$explained$time[2], 2),"%)"))
# 
# l <- subset(loadings$group, PC == 1)
# glg_1 <- tplot_loading(df = l, x = l$covars, y = l$loading) + 
#   labs(x = "Variable", y = paste0("Pr Comp. 1 (",round(100*loadings$explained$group[1], 2),"%)"))
# 
# l <- subset(loadings$group, PC == 2)
# glg_2 <- tplot_loading(df = l, x = l$covars, y = l$loading) + 
#   labs(x = "Variable", y = paste0("Pr. Comp. 2 (",round(100*loadings$explained$group[2], 2),"%)"))
# 
# ggpubr::ggarrange(
#   ggpubr::ggarrange(gst_1, glt_1, gsg_1, glg_1, nrow = 1, widths = c(2,3,2,3), common.legend = TRUE, legend = "none"),
#   ggpubr::ggarrange(gst_2, glt_2, gsg_2, glg_2, nrow = 1, widths = c(2,3,2,3), common.legend = TRUE, legend = "bottom"),
#   nrow = 2
# )
```
</div>

The plot function can also be given a theme or other objects directly,

<div class="fold s">
```{r ALASCA_custom_plot_2}
plot(res.simple, myTheme = theme_dark())
```
</div>

or we can even specify several features if we want,

<div class="fold s">
```{r ALASCA_custom_plot_3}
plot(res.simple, myTheme = list(theme_dark(), scale_colour_viridis_d()))
```
</div>

Note that we can also call the various parts of the plot directly, and apply styles as we wish, for example

<div class="fold s">
```{r ALASCA_custom_plot_4}
ggpubr::ggarrange(
  ggpubr::ggarrange(plot(res.simple, effect = "time", only = "score") + theme_dark(), 
                    plot(res.simple, effect = "time", only = "loading") + theme_dark(), 
                    plot(res.simple, effect = "group", only = "score") + theme_dark(), 
                    plot(res.simple, effect = "group", only = "loading") + theme_dark(), 
                    nrow = 1, widths = c(2,3,2,3), 
                    common.legend = TRUE, legend = "none"),
  ggpubr::ggarrange(plot(res.simple, effect = "time", only = "score", component = 2) + theme_dark(), 
                    plot(res.simple, effect = "time", only = "loading", component = 2) + theme_dark(), 
                    plot(res.simple, effect = "group", only = "score", component = 2) + theme_dark(), 
                    plot(res.simple, effect = "group", only = "loading", component = 2) + theme_dark(), 
                    nrow = 1, widths = c(2,3,2,3), 
                    common.legend = TRUE, legend = "bottom"),
  nrow = 2
)
```
</div>

### Robustness testing

We often need to now how robust our results are. In this package, we use leave-one-out jack-knifing where we divide our participants into `nValFold` groups and leave out one of the groups. Note that the group proportions are kept relatively stable. We repeat this process `nValRuns` times and use the 2.5 and 97.5 percentiles as our uncertainty estimate. If you know that you are going to do this, you can set `validate = TRUE` when you run the ALASCA model the first time. This usually takes some time (but it is extra slow in rmarkdown, so don't be scared, outside knitr it used less than two-three seconds per run on my computer). Also note that we need to specify which column that contains participant id with the argument `partid`. We can also call validation after having initialized the ALASCA object:

```{r robustness}
res.simple$nValRuns <- 20 # you should use more, but this is simply an example

res.simple <- validate(res.simple, participantColumn = "partid")

plot(res.simple)

plot(res.simple, component = 2)
```

<div class="alert alert-info">
<strong>Why no p value?</strong> At the moment, `ALASCA()` does not feature p values, in part because the interpretation might not be straight forward. Instead, we perform **robustness testing**. This is implemented with jack-knifing: divide your participants into *k* groups, and select *k-1* of them. As a default, the participants are stratified by group so that the group proportions are kept constant. You can provide custom stratification groups by specifying a column name, eg. stratGroup, with `stratificationGroup = stratGroup` (Not yet implemented). Nest, we repeat the ALASCA analysis on this subgroup. After repeating this, we collect 2.5 and 97.5 percentiles of scores and loadings to see how stable they are when the underlying data is modified a bit.
</div>

### Alternative models

#### Interaction only

If our experiment was an intervention, then we can assume that the groups were equal at baseline (time 1). Then we can skip the main group effect and only use time and interaction, like this:

```{r ALASCA_model_2}
model.formula <- value ~ time + time:group + (1|partid)
res.mod2 <- ALASCA(df = df, formula = model.formula)
plot(res.mod2)
plot(res.mod2, component = 2)
```

As you can see, it is quite similar to the previous model. Why are they different at the first time point? Well, we can look at the model

```{r ALASCA_model_2_summary}
summary(res.mod2$regr.model[[1]])
```

And you may notice that there is an interaction term between group and the first time point. To remove it, we have to set `forceEqualBaseline = TRUE`:

```{r ALASCA_model_2b}
model.formula <- value ~ time + time:group + (1|partid)
res.mod2b <- ALASCA(df = df, formula = model.formula, forceEqualBaseline = TRUE)
plot(res.mod2b)
plot(res.mod2b, component = 2)
```

#### Combined time and group effects

Sometimes it doesn't make sense to use a reference group -- for example if we didn't have any controls and the first group had a salt-free diet. We can then use a single PCA instead of separating time and group effects:

```{r ALASCA_model_3}
model.formula <- value ~ time*group + (1|partid)
res.mod3 <- ALASCA(df = df, formula = model.formula, separateTimeAndGroup = FALSE)
plot(res.mod3)
plot(res.mod3, component = 2)
```


#### Sum coding

When we don't have any reference group, it may also make sense to use sum coding instead of ordinary contrast coding:

```{r ALASCA_model_3s}
model.formula <- value ~ time*group + (1|partid)
res.mods <- ALASCA(df = df, formula = model.formula, separateTimeAndGroup = FALSE, useSumCoding = TRUE)
plot(res.mods)
plot(res.mods, component = 2)
```

#### Covariates

This time we want to adjust for age and not include it in the ASCA itself;

<div class="fold s">
```{r ALASCA_model_4}
age <- subset(df, variable == "Age")$value
df <- subset(df, variable != "Age")
df$age <- age
model.formula <- value ~ time*group + age + (1|partid)
res.mod4 <- ALASCA(df = df, formula = model.formula)
plot(res.mod4)
plot(res.mod4, component = 2)

do.call(
  ggpubr::ggarrange,
  c(plotPred(res.mod4, variable = c("BMI", "Happiness", "Anger", "HDL")), 
                      common.legend = TRUE, legend = "bottom")
  )
```
</div>

We can see that age is no longer part of the output, and HDL (which was correlated with age) has no longer a strong loading. To see how age is affecting out measurements, we can use `plotCovar()` like this

```{r}
plotCovar(res.mod4, covar = "age", tlab = "Age")
```

## Diagnostics

### Normal distribution of residuals

RM-ASCA+ is built on linear mixed models, and one should check that the residuals approach normal distribution. With `ALASCA()`, the LMM objects can be found as

```{r lmm_info}
summary(res.simple$regr.model[[1]])
```

We can also assess the residuals,

```{r lmm_residuals}
plot(density(residuals(res.simple, variable = "BMI")[[1]]), main = "BMI")

qqnorm(residuals(res.simple, variable = "BMI")[[1]], main = "BMI") 
qqline(residuals(res.simple, variable = "BMI")[[1]])
```

<div class="alert alert-info">
<strong>Why do I need to write `[[1]]` after `residuals.ALASCA()`?</strong> Because a *list* is returned, even though you only specify a single variable, and you have to look at the first item in the list. If you specify several variables, say `variable = c("BMI", "glucose", "Age")`, you must specify if you want to look at BMI (`[[1]]`), glucose (`[[2]]`) or age (`[[3]]`).
</div>

### P values

As you probably noticed above, you can use the `summary()` function to view the p values from your LMMs, but a shortcut is

```{r lmm_pvalues}
head(res.simple$RegressionCoefficients)
```

## References